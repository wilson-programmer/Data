{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/wilsonwaha/endoscopy-detection-action?scriptVersionId=138677789\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-16T11:44:05.219534Z","iopub.execute_input":"2022-07-16T11:44:05.22Z","iopub.status.idle":"2022-07-16T11:44:05.55084Z","shell.execute_reply.started":"2022-07-16T11:44:05.219907Z","shell.execute_reply":"2022-07-16T11:44:05.549655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importation des Bibliotheques**","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# pour divisez les donnees en sous-ensembles d'entraînement et de test aléatoires.\nfrom sklearn.model_selection import train_test_split\n\n# pour calculer la matrice de confusion\nfrom sklearn.metrics import confusion_matrix\n\n# pour réduisez le taux d'apprentissage lorsqu'une métrique a cessé de s'améliorer.\nfrom keras.callbacks import ReduceLROnPlateau\n\n# pour pouvoir importer utiliser le modèle pré-entraîner VGG19\nfrom tensorflow.keras.applications import VGG19\n\n\n# pour convertit le vecteur (y) de classe (entiers) en matrice de plusieurs classes.\nfrom tensorflow.keras.utils import to_categorical\n\n# pour générer de nouvelles images\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# pour initialise le methode de le decente gradien\nfrom tensorflow.keras.optimizers import SGD, Adam\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Flatten, Dense, Dropout","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:34:11.353978Z","iopub.execute_input":"2022-07-16T15:34:11.354322Z","iopub.status.idle":"2022-07-16T15:34:11.361217Z","shell.execute_reply.started":"2022-07-16T15:34:11.354293Z","shell.execute_reply":"2022-07-16T15:34:11.360209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. **Demande de suppression du model sauvegardé:**\n\nComme l'apprentissage prend du temps on a sauvegardé le model, on peut le supprimer pour # reprendre l'apprentissage du model a 0.\n\nSi on répond à la demande de suppression par:\n\n* entre Y: pour suppression du model\n* n'importe quel caractère: ne pas supprimer","metadata":{}},{"cell_type":"code","source":"if os.path.isdir(\"./saved_model\"):\n    try:\n        strr= input('delete saved model? Y/N')\n        if strr == 'Y' or strr == 'y':\n            !rm -rf \"./saved_model\"\n            print(\"deleted model\")\n        else:\n            print('the old model will be used')\n    except:\n        \n        pass","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:34:14.524459Z","iopub.execute_input":"2022-07-16T15:34:14.525142Z","iopub.status.idle":"2022-07-16T15:34:20.143429Z","shell.execute_reply.started":"2022-07-16T15:34:14.525102Z","shell.execute_reply":"2022-07-16T15:34:20.142397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1. Configurer le dataset**","metadata":{}},{"cell_type":"markdown","source":"## **1.1 Telecharger le dataset**","metadata":{}},{"cell_type":"markdown","source":"Ici on vérifie l'environnement d'exécution du notebook (`Google Colab` ou `kaggle`), pour éviter de télécharger le dataset en local car sa taille dépasse les 1Gb.\n\nPour ce faire on utilise une des platforme (`Google Colab` ou `kaggle`) pour exécuter notebook sans télécharge le dataset sur notre machine.\n\nPar ailleurs `Google Colab` et `Kaggle` offre l'option d'exécution avec GPU, ce qui accélérer considérablement les opérations d'apprentissage automatique.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Les fonctions utiliser pour recuperer le dataset\n\"\"\"\n\n# ----------------------------------------------------------------------\ndef in_colab():\n    \"\"\"\n        détecter si le notebook s'exécute sur colab\n\n    Returns:\n        booleen: vrai ou faux\n    \"\"\"\n    try:\n        import google.colab\n\n        IN_COLAB = True\n    except:\n        IN_COLAB = False\n    return IN_COLAB\n\n\n# -----------------------------------------------------------------------\ndef in_kaggle():\n    \"\"\"\n        détecter si le notebook s'exécute sur Kaggle\n\n    Returns:\n        booleen: vrai ou faux\n    \"\"\"\n    import os\n\n    return os.path.isdir(\"../input\") and os.path.isdir(\"/kaggle/working\")\n\n\n# -------------------------------------------------------------------------\ndef download_dataset(URL, dataset_name):\n    \"\"\"\n        telecharge le dataset ddepuis un lien donné\n\n    Args:\n        URL (str): le lien pour telecherge le dataset\n        dataset_name (str): le nom du dataset (kvasir-dataset)\n\n    Returns:\n        str: le chemin du dataset telecharger\n    \"\"\" \"\"\"\"\"\"\n    import tensorflow as tf\n    import os\n\n    path_to_zip = tf.keras.utils.get_file(\n        f\"{dataset_name}.zip\", origin=URL, extract=True\n    )\n    path = os.path.join(os.path.dirname(path_to_zip), dataset_name)\n    return path","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:34:24.220886Z","iopub.execute_input":"2022-07-16T15:34:24.221232Z","iopub.status.idle":"2022-07-16T15:34:24.229185Z","shell.execute_reply.started":"2022-07-16T15:34:24.221203Z","shell.execute_reply":"2022-07-16T15:34:24.228074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_dir = \"../input/endoscopy-data-modify/endoscopy_dataset\"\nprint(f\"Le dataset est disponible sur :\" ,dataset_dir)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:34:29.506998Z","iopub.execute_input":"2022-07-16T15:34:29.507349Z","iopub.status.idle":"2022-07-16T15:34:29.512243Z","shell.execute_reply.started":"2022-07-16T15:34:29.50732Z","shell.execute_reply":"2022-07-16T15:34:29.511292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.2. Definir les categories du dataset**","metadata":{}},{"cell_type":"markdown","source":"Ici on définit les catégories (classes) du dataset a partir des noms de dossiers du dataset.\nLe dataset se compose de 8 dossiers (un pour chaque classe) nommer suivant leur catégorie, chaque dossier comporte 100 images.","metadata":{}},{"cell_type":"code","source":"def get_dataCategories(dataset_dir):\n    \"\"\"\n        optenire les categories (classe) du dataset a partire du chemin du dataset \n        en question.\n\n    Args:\n        dataset_dir (str): chemin du dataset\n\n    Returns:\n        list: liste des categories\n        list: liste des nombre de fichier par categories\n    \"\"\"\n    import glob\n\n    categories = []\n    for folder_name in os.listdir(dataset_dir):\n        if os.path.isdir(os.path.join(dataset_dir, folder_name)):\n            nbr_files = len(\n                glob.glob(os.path.join(dataset_dir, folder_name) + \"/*.jpg\")\n            )\n            categories.append(np.array([folder_name, nbr_files]))\n\n    categories.sort(key=lambda a: a[0])\n    cat = np.array(categories)\n\n    return list(cat[:, 0]), list(cat[:, 1])\n\n\n\ndataset_dir =\"../input/endoscopy-data-modify/endoscopy_dataset\"\ncategories, nbr_files = get_dataCategories(dataset_dir)\n\n# Create DataFrame\ndf = pd.DataFrame({\"categorie\": categories, \"numbre of files\": nbr_files})\nprint(\"number of categories: \", len(categories))\ndf","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:34:33.75531Z","iopub.execute_input":"2022-07-16T15:34:33.755647Z","iopub.status.idle":"2022-07-16T15:34:33.785314Z","shell.execute_reply.started":"2022-07-16T15:34:33.755617Z","shell.execute_reply":"2022-07-16T15:34:33.784368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.3 créer l'ensemble des features X ainsi que les labels Y**","metadata":{}},{"cell_type":"markdown","source":"pour se faire on lit chaque image du dataset et on la met dans `X`, et on sauvegarde la classe de l'image lue dans `y`.\n\n**Remarque**: on redimensionne les images lues en 100x100 pour accélérer l'étape d'apprentissage","metadata":{}},{"cell_type":"code","source":"def create_dataset(datadir, categories,img_wid, img_high):\n    X, y = [], []\n    for category in categories:\n        path = os.path.join(datadir, category)\n        class_num = categories.index(category)\n        for img in os.listdir(path):\n            try:\n                img_array = cv2.imread(os.path.join(path, img))\n                ima_resize_rgb = cv2.resize(img_array, (img_wid, img_high))\n\n                X.append(ima_resize_rgb)\n                y.append(class_num)\n\n            except Exception as e:\n                pass\n\n    y = np.array(y)\n    X = np.array(X).reshape(y.shape[0], img_wid, img_wid, 3)\n    return X, y\n\n\n\n\n#img_wid, img_high = 100, 100\nimg_wid, img_high = 256, 256\n\nX, y = create_dataset(dataset_dir, categories, img_wid, img_high)\n\nprint(f\"X: {X.shape}\")\nprint(f\"y: {y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:34:44.09427Z","iopub.execute_input":"2022-07-16T15:34:44.09461Z","iopub.status.idle":"2022-07-16T15:34:49.092848Z","shell.execute_reply.started":"2022-07-16T15:34:44.09458Z","shell.execute_reply":"2022-07-16T15:34:49.090743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.4 Afficher une image aleatoire pour chaque categories**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nst, end = 0, 1\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    idx = np.random.randint(st, end)\n    st = end + 1\n    end = (i + 2) * 100\n    # plt.imshow(X[idx][:,:,::-1])\n    plt.imshow(X[idx][:, :, ::-1])\n    plt.title(f\"{i}. {categories[y[idx]]}\")\n    plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:34:54.57538Z","iopub.execute_input":"2022-07-16T15:34:54.575758Z","iopub.status.idle":"2022-07-16T15:34:55.041447Z","shell.execute_reply.started":"2022-07-16T15:34:54.575721Z","shell.execute_reply":"2022-07-16T15:34:55.040569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Création du model et apprentissage**","metadata":{}},{"cell_type":"markdown","source":"### **2.1. Creer l'ensemble d'apprentissage, test et de validation**\nDans cette etap on cree:\n- l'ensemble d'entrainement `x_train`/`y_train`\n- l'ensemble de test `x_test`/`y_test`\n- l'ensemble de validation `x_val`/`y_va`","metadata":{}},{"cell_type":"markdown","source":"#### **2.1.1. L'ensemble d'entrainment `(X/y)train` et de test `(X/y)test`**\nOn divise les données (X, y) en ensemble d'entrainement et de test en utilisant 80 % des données pour l'apprentissage et les 20 % restants pour les tests.","metadata":{}},{"cell_type":"code","source":"# en convertie y en format scaler\nY = np.reshape(y, (len(y), 1))\n\n# split dataset to train and test set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, Y, train_size=0.8, random_state=42\n)\nprint(f\"X_train: {X_train.shape}\")\nprint(f\"t_train: {y_train.shape}\")\nprint(f\"X_test: {X_test.shape}\")\nprint(f\"y_test: {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:02.587078Z","iopub.execute_input":"2022-07-16T15:35:02.58742Z","iopub.status.idle":"2022-07-16T15:35:02.641367Z","shell.execute_reply.started":"2022-07-16T15:35:02.587389Z","shell.execute_reply":"2022-07-16T15:35:02.640363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **2.1.2. Creer l'ensemble de validation `x_val/y_val`**","metadata":{}},{"cell_type":"markdown","source":"Ici,  on divise 30% de l'ensemble d'entrainement en ensemble de validation","metadata":{}},{"cell_type":"code","source":"# defining training and test sets\nx_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\nx_test = X_test\n\n# Dimension of the dataset\nprint(f\"x_train:{x_train.shape},  y_train:{y_train.shape}\")\nprint(f\"x_val:{x_val.shape},  y_val:{y_val.shape}\")\nprint(f\"x_test:{x_test.shape},  y_test:{y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:15.06746Z","iopub.execute_input":"2022-07-16T15:35:15.067818Z","iopub.status.idle":"2022-07-16T15:35:15.11473Z","shell.execute_reply.started":"2022-07-16T15:35:15.067786Z","shell.execute_reply":"2022-07-16T15:35:15.113725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **2.1.3. Encodage à chaud (OneHot Encoding)**","metadata":{}},{"cell_type":"markdown","source":"Nous devons faire un encodage à chaud (OneHot Encoding) avec `to_categorical`, pour transformer l'ensemble des lables (`y_train`, `y_val` et `y_test`) de tel sorte à avoir un vecteur pour chaque exemple, car nous avons 8 classes et nous devrions nous attendre à ce que la forme de (`y_train`, `y_val` et `y_test`) passe de 1 à 8","metadata":{}},{"cell_type":"code","source":"# One Hot Encoding\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)\ny_test = to_categorical(y_test)\n\n# Verifying the dimension after one hot encoding\nprint(f\"x_train:{x_train.shape},  y_train:{y_train.shape}\")\nprint(f\"x_train:{x_val.shape},  y_train:{y_val.shape}\")\nprint(f\"x_train:{x_test.shape},  y_train:{y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:19.814915Z","iopub.execute_input":"2022-07-16T15:35:19.81581Z","iopub.status.idle":"2022-07-16T15:35:19.823989Z","shell.execute_reply.started":"2022-07-16T15:35:19.81576Z","shell.execute_reply":"2022-07-16T15:35:19.822899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.2. Generation d'images**","metadata":{}},{"cell_type":"markdown","source":"Ici, nous allons effectuer l'augmentation des données d'image. Il s'agit de la technique utilisée pour augmenter la taille d'un ensemble de données d'apprentissage en créant des versions modifiées d'images dans l'ensemble de données. La création de ces images modifié s'effectue en pivotent de manière aléatoire ces images de n'importe quel degré entre 0 et 360.\n\nTout d'abord, nous définirons des instances individuelles d'ImageDataGenerator pour l'augmentation, puis nous les adapterons à chacun des ensembles de données d'entraînement, de test et de validation. ","metadata":{}},{"cell_type":"code","source":"# Image Data Augmentation\ntrain_generator = ImageDataGenerator(\n    rotation_range=2, horizontal_flip=True, zoom_range=0.1\n)\n\nval_generator = ImageDataGenerator(\n    rotation_range=2, horizontal_flip=True, zoom_range=0.1\n)\n\ntest_generator = ImageDataGenerator(\n    rotation_range=2, horizontal_flip=True, zoom_range=0.1\n)\n\n# Fitting the augmentation defined above to the data\ntrain_generator.fit(x_train)\nval_generator.fit(x_val)\ntest_generator.fit(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:25.154348Z","iopub.execute_input":"2022-07-16T15:35:25.154706Z","iopub.status.idle":"2022-07-16T15:35:25.543136Z","shell.execute_reply.started":"2022-07-16T15:35:25.154671Z","shell.execute_reply":"2022-07-16T15:35:25.542133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.3. Telecharger le model pré-entraîné VGG19**","metadata":{}},{"cell_type":"markdown","source":"Maintenant, nous allons instancier le model `VGG19` qui est un réseau de neurones convolutif pré-entraîné en tant que modèle d'apprentissage par transfert.\n\nLe plus grand avantage de ce réseau est que il a été pre-entraîné sur plus d'un million d'images de la base de données ImageNet. \n\nUn réseau pré-entraîné peut classer les images en milliers de catégories d'objets. En raison de cet avantage, nous allons appliquer ce modèle sur notre dataset qui comporte 8 catégories, en lui ajoutent d'autre couches.","metadata":{}},{"cell_type":"code","source":"\"\"\"base_model = VGG19(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(100, 100, 3),\n        classes=y_train.shape[1],)\n # save model\nbase_model.save(\"./saved_model/vgg19_model.h5\")\"\"\"\n\nbase_model = VGG19(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(256, 256, 3),\n        classes=y_train.shape[1],)\n # save model\nbase_model.save(\"./saved_model/vgg19_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:31.154927Z","iopub.execute_input":"2022-07-16T15:35:31.155829Z","iopub.status.idle":"2022-07-16T15:35:31.67093Z","shell.execute_reply.started":"2022-07-16T15:35:31.155789Z","shell.execute_reply":"2022-07-16T15:35:31.669744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nous allons maintenant définir `VGG19` comme une architecture d'apprentissage profond. Pour cela, il sera défini comme un modèle séquentiel de Keras à plusieurs couches denses.","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:36.934313Z","iopub.execute_input":"2022-07-16T15:35:36.934678Z","iopub.status.idle":"2022-07-16T15:35:37.000691Z","shell.execute_reply.started":"2022-07-16T15:35:36.93462Z","shell.execute_reply":"2022-07-16T15:35:36.99961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maintenant, pour ajouter d'autres couches, nous devons voir la dimension de notre modèle.","metadata":{}},{"cell_type":"code","source":"# Model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:41.964203Z","iopub.execute_input":"2022-07-16T15:35:41.96454Z","iopub.status.idle":"2022-07-16T15:35:41.970957Z","shell.execute_reply.started":"2022-07-16T15:35:41.96451Z","shell.execute_reply":"2022-07-16T15:35:41.969846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ajouter des couches dense avec l'activation et la normalisation par lots ","metadata":{}},{"cell_type":"code","source":"model.add(Dense(1024, activation=(\"relu\"), input_dim=512))\nmodel.add(Dense(512, activation=(\"relu\")))\nmodel.add(Dense(256, activation=(\"relu\")))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation=(\"relu\")))\n# model.add(Dropout(.2))\nmodel.add(Dense(y_train.shape[1], activation=(\"softmax\")))","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:46.184887Z","iopub.execute_input":"2022-07-16T15:35:46.185217Z","iopub.status.idle":"2022-07-16T15:35:46.227383Z","shell.execute_reply.started":"2022-07-16T15:35:46.185188Z","shell.execute_reply":"2022-07-16T15:35:46.226474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Verification du model final","metadata":{}},{"cell_type":"markdown","source":"### **2.4. Hyperparametrage**","metadata":{}},{"cell_type":"markdown","source":"Les hyperparamètres sont des paramètres réglables qui nous permettent de contrôler le processus d'entraînement du modèle. Les performances du modèle dépendent fortement des hyperparamètres.","metadata":{}},{"cell_type":"markdown","source":"Comme nous avons défini notre modèle, nous devons maintenant initialiser les hyperparamètres nécessaires pour former le modèle, puis enfin, nous allons compiler notre modèle.","metadata":{}},{"cell_type":"code","source":"\"\"\" Initializing the hyperparameters \"\"\"\n\n# initialise le nombre d'échantillons d'apprentisage\nbatch_size = 100\n\n# initialise le nombre d'iteration\nepochs = 10\n\n# taux d'apprentisage\n#learn_rate = 0.001\n#learn_rate = 0.00001\nlearn_rate = 5.5e-5\n\n# initilisation de la descente du gradient\nsgd = SGD(learning_rate=learn_rate, momentum=0.9, nesterov=False)\n\n# initilisation de la descente du gradient Adam\n#adam = Adam( learning_rate=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\n# compiler le model\nmodel.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:35:56.64252Z","iopub.execute_input":"2022-07-16T15:35:56.643196Z","iopub.status.idle":"2022-07-16T15:35:56.654766Z","shell.execute_reply.started":"2022-07-16T15:35:56.643155Z","shell.execute_reply":"2022-07-16T15:35:56.653766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La reduction de taux d'apprentissage diminue le taux d'apprentissage après un certain nombre d'iteration si le taux d'erreur ne change pas. Ici, grâce à cette technique, nous surveillerons la précision de la validation et si cela cesse de s'améliorer on réduira le taux d'apprentissage de 0,01.","metadata":{}},{"cell_type":"code","source":"# Learning Rate Annealer\nlrr = ReduceLROnPlateau(monitor=\"val_acc\", factor=0.01, patience=3, min_lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:36:03.196098Z","iopub.execute_input":"2022-07-16T15:36:03.196451Z","iopub.status.idle":"2022-07-16T15:36:03.201609Z","shell.execute_reply.started":"2022-07-16T15:36:03.196421Z","shell.execute_reply":"2022-07-16T15:36:03.200374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.5. Entrainement du  model**","metadata":{}},{"cell_type":"markdown","source":"Maintenant, nous commençons à former notre model.","metadata":{}},{"cell_type":"code","source":"# si model a ete deja entrainer et sauvgarde ---> lire le model\n\"\"\"\"if os.path.isfile(\"./saved_model/model.h5\"):\n\n    # lire le model\n    model = load_model(\"./saved_model/model.h5\")\n\n    # si l'historique du model a ete sauvgarde --> lire l'historique\n    if os.path.isfile(\"./saved_model/model_history.npy\"):\n\n        # lire l'historique\n        history = np.load(\"./saved_model/model_history.npy\", allow_pickle=\"TRUE\").item()\n    else:\n        history = None\n\n# si le model n'a pas ete deja entrainer --> entrainer le model\nelse:\n    history = model.fit(\n        # train_generator.flow(x_train, y_train, batch_size= batch_size),\n        x_train,\n        y_train,\n        epochs=epochs,\n        steps_per_epoch=x_train.shape[0] // batch_size,\n        validation_data=val_generator.flow(x_val, y_val, batch_size=batch_size),\n        validation_steps=250,\n        callbacks=[lrr],\n        verbose=1,\n    )\n    history = history.history\n\n    # save model\n    np.save(\"./saved_model/model_history.npy\", history)\n    model.save(\"./saved_model/model.h5\")\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:33:50.96874Z","iopub.execute_input":"2022-07-16T15:33:50.969696Z","iopub.status.idle":"2022-07-16T15:33:50.977783Z","shell.execute_reply.started":"2022-07-16T15:33:50.969623Z","shell.execute_reply":"2022-07-16T15:33:50.976712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n        # train_generator.flow(x_train, y_train, batch_size= batch_size),\n        x_train,\n        y_train,\n        epochs=epochs,\n        steps_per_epoch=x_train.shape[0] // batch_size,\n        validation_data=val_generator.flow(x_val, y_val, batch_size=batch_size),\n        validation_steps=250,\n        callbacks=[lrr],\n        verbose=1,\n    )\nhistory = history.history\n # save model\nnp.save(\"./saved_model/model_history.npy\", history)\nmodel.save(\"./saved_model/model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:36:10.934885Z","iopub.execute_input":"2022-07-16T15:36:10.935238Z","iopub.status.idle":"2022-07-16T15:37:34.923673Z","shell.execute_reply.started":"2022-07-16T15:36:10.935197Z","shell.execute_reply":"2022-07-16T15:37:34.922476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3. Evaluation du model**","metadata":{}},{"cell_type":"markdown","source":"### **3.1. Evaluer la précision sur l'ensemble de test**","metadata":{}},{"cell_type":"markdown","source":"Evaluer la précision ainsi que la perte du model sur l'ensemble de test","metadata":{}},{"cell_type":"code","source":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", round(score[0], 3))\nprint(\"Test accuracy:\", round(score[1], 3))","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:40:35.601961Z","iopub.execute_input":"2022-07-16T15:40:35.602589Z","iopub.status.idle":"2022-07-16T15:40:36.309089Z","shell.execute_reply.started":"2022-07-16T15:40:35.602551Z","shell.execute_reply":"2022-07-16T15:40:36.308148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy","metadata":{}},{"cell_type":"markdown","source":"courbe montrant l'evolution de l'accuracy","metadata":{}},{"cell_type":"code","source":"plt.plot(history['accuracy'])\nplt.title(\"Evolution de l'accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:40:51.88274Z","iopub.execute_input":"2022-07-16T15:40:51.883591Z","iopub.status.idle":"2022-07-16T15:40:52.061591Z","shell.execute_reply.started":"2022-07-16T15:40:51.883541Z","shell.execute_reply":"2022-07-16T15:40:52.060618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history['loss'])\nplt.title(\"Evolution de la perte\")","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:40:57.102014Z","iopub.execute_input":"2022-07-16T15:40:57.103092Z","iopub.status.idle":"2022-07-16T15:41:36.661529Z","shell.execute_reply.started":"2022-07-16T15:40:57.103047Z","shell.execute_reply":"2022-07-16T15:41:36.660584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3.2. Matrice de confusion**","metadata":{}},{"cell_type":"markdown","source":"Enfin, nous visualiserons les performances de classification sur des données de test à l'aide de matrices de confusion.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Les fonctions utilise affiche la matrice de confusion\n\"\"\"\n\n# ----------------------------------------------------------------------\ndef cm_plt(ax, cm, classes, cmap, title, normalize):\n    \"\"\"\n       affiche la matrice de confusion appartire de l'ax entre\n\n    Args:\n        ax (plt): ax qui sert a affiche la matrice\n        cm (numpy): matrice de confusion\n        classes (list): liste des classes\n        cmap (plt): couluer de la matrice\n        title (str): titre de la matrice\n        normalize (booleen): vrai affichre cm normalisee\n\n    Returns:\n        plt: l'ax qui sera affiche\n    \"\"\"\n    im = ax.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(\n        xticks=np.arange(cm.shape[1]),\n        yticks=np.arange(cm.shape[0]),\n        # ... and label them with the respective list entries\n        xticklabels=classes,\n        yticklabels=classes,\n        title=title,\n        ylabel=\"True label\",\n        xlabel=\"Predicted label\",\n    )\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = \".2f\" if normalize else \"d\"\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(\n                j,\n                i,\n                format(cm[i, j], fmt),\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n            )\n\n    return ax\n\n\n# ----------------------------------------------------------------------\n# Defining function for confusion matrix plot\ndef plt_confusion_mat(cm, classes, fig_size, cmap=plt.cm.Blues):\n    \"\"\"\n        afficher la cm normalisee et non normalisee\n\n    Args:\n        cm (numpy): matrice de confusion\n        classes (list): liste des classes\n        fig_size (_type_): _description_\n        cmap (plt, optional): couluer de la matrice. Defaults to plt.cm.Blues.\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=fig_size)\n    ax1 = cm_plt(\n        ax1,\n        cm,\n        classes,\n        cmap,\n        title=\"Confusion matrix, without normalization\",\n        normalize=False,\n    )\n\n    cmn = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n    ax2 = cm_plt(\n        ax2,\n        cmn,\n        classes,\n        cmap,\n        title=\"Normalized confusion matrix\",\n        normalize=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:42:07.379776Z","iopub.execute_input":"2022-07-16T15:42:07.380216Z","iopub.status.idle":"2022-07-16T15:42:07.402761Z","shell.execute_reply.started":"2022-07-16T15:42:07.380176Z","shell.execute_reply":"2022-07-16T15:42:07.401601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prédir les classes de l'ensemble de test","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Making prediction\ny_pred = np.argmax(model.predict(x_test), axis=1)\ny_true = np.argmax(y_test, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:42:11.505707Z","iopub.execute_input":"2022-07-16T15:42:11.506485Z","iopub.status.idle":"2022-07-16T15:42:12.145634Z","shell.execute_reply.started":"2022-07-16T15:42:11.506448Z","shell.execute_reply":"2022-07-16T15:42:12.144669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Affiche la matrice de confusion normalisée et non normalisée.\n\nNous verrons le nombre exact de classifications correctes et incorrectes à l'aide de la matrice de confusion non normalisée, puis nous verrons la même chose en pourcentage à l'aide de la matrice de confusion normalisée.\n\nComme on peut le voir en classant les images en 8 classes, le modèle a donné une précision minimale de 79% et une précision maximale de 97%. Nous pouvons affiner davantage les paramètres de formation et réentraîner notre modèle pour voir toute mise à l'échelle possible dans la classification","metadata":{}},{"cell_type":"code","source":"# get confusion matrix\nconfuision_mat = confusion_matrix(y_true, y_pred)\n# plot confusion_mat\nplt_confusion_mat(confuision_mat, classes=categories, fig_size=(20, 7))","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:42:45.704882Z","iopub.execute_input":"2022-07-16T15:42:45.705223Z","iopub.status.idle":"2022-07-16T15:42:46.533986Z","shell.execute_reply.started":"2022-07-16T15:42:45.705195Z","shell.execute_reply":"2022-07-16T15:42:46.532937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3.3. Predicion au hasard**","metadata":{}},{"cell_type":"markdown","source":"Prédire des images choisies au hasard et compare la prédiction avec la vérité terrain","metadata":{}},{"cell_type":"code","source":"def predict_categorie_img(img, model, categories):\n    \"\"\"\n        predire la classe d'une image donnee\n\n    Args:\n        img (numpay): l'image donnee\n        model (_type_): le model utilise pour la prediction\n        categories (list): liste des classes\n\n    Raises:\n        TypeError: erreur si l'image n'est pas en RGB (image_dim < 3)\n\n    Returns:\n        int: l'index de la classes predie\n        str: la classe predie\n    \"\"\"    \n    try:\n        img = img[None, :, :, :]\n    except:\n        raise TypeError(\"test image dimension != 3\")\n    predict = model.predict(img)\n    idx_cat = np.argmax(predict, axis=1)[0]\n    return idx_cat, categories[idx_cat]\n\n\n\n\n\nplt.figure(figsize=(20, 8))\nfor i in range(10):\n    idx = np.random.randint(len(y))\n    img = X[idx]\n    pred_class = predict_categorie_img(img, model, categories)\n    true_class = y[idx], categories[y[idx]]\n\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(img[:, :, ::-1])\n    plt.title(f\"Pred:[{pred_class}]\\nTrue:[{true_class}]\")\n    plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-16T15:43:13.671758Z","iopub.execute_input":"2022-07-16T15:43:13.672378Z","iopub.status.idle":"2022-07-16T15:43:14.922162Z","shell.execute_reply.started":"2022-07-16T15:43:13.672342Z","shell.execute_reply":"2022-07-16T15:43:14.920172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a helper function to visualize batches:","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-03T13:25:22.677016Z","iopub.execute_input":"2022-07-03T13:25:22.677567Z","iopub.status.idle":"2022-07-03T13:25:28.899393Z","shell.execute_reply.started":"2022-07-03T13:25:22.677523Z","shell.execute_reply":"2022-07-03T13:25:28.89835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-03T13:27:59.275076Z","iopub.execute_input":"2022-07-03T13:27:59.275574Z","iopub.status.idle":"2022-07-03T13:27:59.286326Z","shell.execute_reply.started":"2022-07-03T13:27:59.275538Z","shell.execute_reply":"2022-07-03T13:27:59.285238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-03T13:28:24.924563Z","iopub.execute_input":"2022-07-03T13:28:24.925077Z","iopub.status.idle":"2022-07-03T13:28:24.93371Z","shell.execute_reply.started":"2022-07-03T13:28:24.92503Z","shell.execute_reply":"2022-07-03T13:28:24.932271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-03T13:28:53.150433Z","iopub.execute_input":"2022-07-03T13:28:53.150848Z","iopub.status.idle":"2022-07-03T13:28:53.165534Z","shell.execute_reply.started":"2022-07-03T13:28:53.150816Z","shell.execute_reply":"2022-07-03T13:28:53.164627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-03T13:30:13.071126Z","iopub.execute_input":"2022-07-03T13:30:13.071614Z","iopub.status.idle":"2022-07-03T13:30:13.082013Z","shell.execute_reply.started":"2022-07-03T13:30:13.071576Z","shell.execute_reply":"2022-07-03T13:30:13.081073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-03T13:35:56.390191Z","iopub.execute_input":"2022-07-03T13:35:56.390697Z","iopub.status.idle":"2022-07-03T13:35:57.188829Z","shell.execute_reply.started":"2022-07-03T13:35:56.390658Z","shell.execute_reply":"2022-07-03T13:35:57.187925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}